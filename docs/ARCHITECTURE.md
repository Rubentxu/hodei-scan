# Arquitectura T√©cnica de hodei-scan
## Sistema de An√°lisis de C√≥digo Nativo con Motor de Reglas Determinista

**Versi√≥n:** 2.0
**Fecha:** 10 de noviembre de 2025
**Estado:** ‚úÖ Documentado
**Scope:** Arquitectura completa del sistema

---

## üìã Resumen Ejecutivo

hodei-scan es un sistema de an√°lisis de c√≥digo est√°tico nativo en Rust dise√±ado para competir con SonarQube, ofreciendo 2x-5x mejor performance, 5x menos uso de memoria y determinismo O(n) garantizado. La arquitectura sigue principios de **Hexagonal Architecture** con **Clean Architecture** y **SOLID** principles.

**Diferenciadores Arquitect√≥nicos Clave:**
- ‚úÖ **Cedar-Inspired Rule Engine**: Motor de reglas determinista <2ms
- ‚úÖ **Sin contradicciones**: Eliminated LSP dependency del motor core
- ‚úÖ **An√°lisis Sem√°ntico Profundo**: DFA, CFG, taint tracking por lenguaje
- ‚úÖ **Extensibilidad WASM**: Sandbox para reglas enterprise
- ‚úÖ **An√°lisis Incremental**: Sub-segundo feedback en tiempo real

---

## üèóÔ∏è Vista General de la Arquitectura

### Capas Arquitect√≥nicas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRESENTATION LAYER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   CLI        ‚îÇ ‚îÇ   Web API    ‚îÇ ‚îÇ   GitHub/GitLab      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Interface  ‚îÇ ‚îÇ   (Axum)     ‚îÇ ‚îÇ   Integrations       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   APPLICATION LAYER                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Analysis    ‚îÇ ‚îÇ  SCA Engine  ‚îÇ ‚îÇ  Coverage Engine     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Coordinator ‚îÇ ‚îÇ              ‚îÇ ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                              ‚îÇ                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Quality Gate ‚îÇ ‚îÇ PR Decoration‚îÇ ‚îÇ  Debt Calculator     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Evaluator   ‚îÇ ‚îÇ  Engine      ‚îÇ ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DOMAIN LAYER                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Cedar Rule   ‚îÇ ‚îÇ  Taint       ‚îÇ ‚îÇ  Portfolio           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Engine      ‚îÇ ‚îÇ  Analysis    ‚îÇ ‚îÇ  Management          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                    ‚îÇ                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ  ‚îÇ  Security   ‚îÇ  ‚îÇ  Language          ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ  Analyzer   ‚îÇ  ‚îÇ  Analyzers         ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ  (SAST)     ‚îÇ  ‚îÇ  (Rust, Go, TS)    ‚îÇ                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 INFRASTRUCTURE LAYER                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ PostgreSQL   ‚îÇ ‚îÇ    Redis     ‚îÇ ‚îÇ   NATS Messaging     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Database   ‚îÇ ‚îÇ    Cache     ‚îÇ ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                              ‚îÇ                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  File        ‚îÇ ‚îÇ  Search      ‚îÇ ‚îÇ   gRPC Workers       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  System      ‚îÇ ‚îÇ  (Tantivy)   ‚îÇ ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Diagrama de Componentes

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ        hodei-scan System            ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                           ‚îÇ                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CLI Client     ‚îÇ      ‚îÇ   Web Service   ‚îÇ      ‚îÇ  CI/CD Integrator ‚îÇ
‚îÇ                  ‚îÇ      ‚îÇ   (Axum)        ‚îÇ      ‚îÇ                   ‚îÇ
‚îÇ ‚Ä¢ analyze        ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ ‚Ä¢ GitHub Actions  ‚îÇ
‚îÇ ‚Ä¢ sca            ‚îÇ      ‚îÇ ‚Ä¢ REST API      ‚îÇ      ‚îÇ ‚Ä¢ GitLab CI       ‚îÇ
‚îÇ ‚Ä¢ report         ‚îÇ      ‚îÇ ‚Ä¢ WebSocket     ‚îÇ      ‚îÇ ‚Ä¢ Bitbucket       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚Ä¢ Static Files  ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     API Gateway Layer        ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                    ‚îÇ  ‚îÇAuth/JWT ‚îÇ ‚îÇRate Limit ‚îÇ  ‚îÇ
                    ‚îÇ  ‚îÇMiddleware‚îÇ ‚îÇMiddleware ‚îÇ  ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Analysis Pipeline Core     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                         ‚îÇ                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Language Parser   ‚îÇ  ‚îÇ Cedar Rule      ‚îÇ  ‚îÇ   Security Engine     ‚îÇ
‚îÇ                     ‚îÇ  ‚îÇ Engine          ‚îÇ  ‚îÇ                       ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇTree ‚îÇ ‚îÇOxC     ‚îÇ ‚îÇ  ‚îÇ ‚Ä¢ DSL Parser    ‚îÇ  ‚îÇ ‚îÇTaint‚îÇ ‚îÇOWASP      ‚îÇ ‚îÇ
‚îÇ ‚îÇSitter‚îÇ ‚îÇParser  ‚îÇ ‚îÇ  ‚îÇ ‚Ä¢ Rule Index   ‚îÇ  ‚îÇ ‚îÇAnal.‚îÇ ‚îÇTop 10     ‚îÇ ‚îÇ
‚îÇ ‚îÇRust ‚îÇ ‚îÇTS/JS  ‚îÇ ‚îÇ  ‚îÇ ‚Ä¢ Evaluator     ‚îÇ  ‚îÇ ‚îÇ     ‚îÇ ‚îÇ           ‚îÇ ‚îÇ
‚îÇ ‚îÇGo   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚Ä¢ WASM Runtime  ‚îÇ  ‚îÇ ‚îÇ     ‚îÇ ‚îÇCWE Top 25 ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                         ‚îÇ                         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ      Data Layer           ‚îÇ
                    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ ‚îÇPostgreSQL‚îÇ ‚îÇ  Redis   ‚îÇ ‚îÇ
                    ‚îÇ ‚îÇ  Main DB ‚îÇ ‚îÇ  Cache   ‚îÇ ‚îÇ
                    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                    ‚îÇ ‚îÇ  S3     ‚îÇ ‚îÇ  NATS    ‚îÇ ‚îÇ
                    ‚îÇ ‚îÇReports  ‚îÇ ‚îÇMessage   ‚îÇ ‚îÇ
                    ‚îÇ ‚îÇ         ‚îÇ ‚îÇQueue     ‚îÇ ‚îÇ
                    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Componentes Core

### 1. Cedar-Inspired Rule Engine

**Prop√≥sito:** Motor de reglas determinista para static analysis

**Caracter√≠sticas:**
- Evaluaci√≥n de reglas < 2ms
- Determinismo O(n) garantizado
- Paralelizaci√≥n con Rayon
- WASM sandbox para reglas custom

**Arquitectura Interna:**

```rust
// Core Rule Engine
pub struct RuleEngine {
    schema: AnalysisSchema,              // Schema-driven types
    rules: Arc<RwLock<HashMap<RuleId, Box<dyn StaticAnalysisRule>>>>,
    index: Arc<RuleIndex>,               // Fast rule slicing
    evaluation_pool: Arc<rayon::ThreadPool>,
    wasm_runtime: Option<WASMRuntime>,
}

// Rule Definition (DSL Cedar-inspired)
pub struct SonarRule {
    pub id: RuleId,
    pub effect: RuleEffect,              // Permit/Forbid
    pub scope: RuleScope,                // Node type + attributes
    pub conditions: Vec<Condition>,      // AST-based conditions
    pub context: ContextExpression,      // Project/file context
    pub metadata: RuleMetadata,          // Severity, message, fix
}

// Evaluation Flow
impl RuleEngine {
    pub async fn evaluate(
        &self,
        node: &ASTNode,
        context: &AnalysisContext
    ) -> Vec<RuleViolation> {
        // 1. Rule slicing: fast filter
        let relevant_rules = self.index.slice(context);

        // 2. Parallel evaluation
        let violations: Vec<RuleViolation> = relevant_rules
            .par_iter()
            .filter_map(|rule_id| {
                let rule = self.get_rule(rule_id);
                rule.evaluate(node, context)
            })
            .collect();

        violations
    }
}
```

**DSL de Reglas:**

```cedar
// Regla ejemplo: SQL Injection Detection
permit(
    rule: "GO_SQL_INJECTION",
    severity: "critical",
    category: "security"
) on {
    node_type: "binary_expr",
    operator: "+",
    context: {
        language == "go" &&
        right_operand: { node_type: "identifier" }
    }
} when {
    context.is_user_input(right_operand) &&
    context.is_sql_sink(parent_node)
}

forbid(
    rule: "RUST_UNSAFE_NO_COMMENT",
    severity: "warning"
) on {
    node_type: "unsafe_block",
    condition: { !has_safety_comment }
} when {
    context.language == "rust" &&
    context.complexity > 5
}
```

### 2. Language Analyzer Layer

**Prop√≥sito:** An√°lisis sem√°ntico por lenguaje espec√≠fico

**Trait Base:**

```rust
pub trait LanguageAnalyzer: Send + Sync {
    type AST;
    type CFG;
    type DataFlowGraph;

    fn parse(&self, source: &str) -> Result<Self::AST, ParseError>;

    fn build_cfg(&self, ast: &Self::AST) -> Result<Self::CFG, BuildError>;

    fn dataflow_analysis(&self, cfg: &Self::CFG) -> Result<Self::DataFlowGraph, DFAError>;

    fn taint_tracking(&self, dfg: &Self::DataFlowGraph) -> Result<TaintResults, TaintError>;
}
```

**Implementaci√≥n Rust:**

```rust
pub struct RustAnalyzer {
    parser: syn::Parser,
    cfg_builder: RustCFGBuilder,
    dfa: RustDFA,
    taint_tracker: RustTaintTracker,
}

impl LanguageAnalyzer for RustAnalyzer {
    type AST = RustAST;
    type CFG = RustCFG;
    type DataFlowGraph = RustDataFlowGraph;

    fn parse(&self, source: &str) -> Result<Self::AST, ParseError> {
        // 1. Parse with syn
        let syntax_tree = syn::parse_file(source)
            .map_err(|e| ParseError::SynError(e))?;

        // 2. Build AST with additional metadata
        Ok(RustAST {
            functions: syntax_tree.items
                .iter()
                .filter_map(|item| match item {
                    Item::Fn(func) => Some(RustFunction::from_syn(func)),
                    _ => None,
                })
                .collect(),
            structs: /* ... */,
            // ...
        })
    }

    fn build_cfg(&self, ast: &Self::AST) -> Result<Self::CFG, BuildError> {
        self.cfg_builder.build(ast)
    }

    fn dataflow_analysis(&self, cfg: &Self::CFG) -> Result<Self::DataFlowGraph, DFAError> {
        self.dfa.analyze(cfg)
    }

    fn taint_tracking(&self, dfg: &Self::DataFlowGraph) -> Result<TaintResults, TaintError> {
        self.taint_tracker.track_sources_to_sinks(dfg)
    }
}
```

### 3. Security Analysis Engine (SAST)

**Prop√≥sito:** An√°lisis de vulnerabilidades de seguridad

**Arquitectura:**

```rust
pub struct SecurityAnalyzer {
    rule_engine: Arc<RuleEngine>,
    taint_analyzer: TaintAnalyzer,
    framework_detector: FrameworkDetector,
}

impl SecurityAnalyzer {
    pub async fn analyze(&self, project: &Project) -> SecurityReport {
        // 1. Detect frameworks
        let frameworks = self.framework_detector.detect(project);

        // 2. Get relevant rules
        let security_rules = self.rule_engine.get_security_rules(&frameworks);

        // 3. Run taint analysis
        let taint_results = self.taint_analyzer.analyze(project);

        // 4. Evaluate rules
        let findings: Vec<SecurityFinding> = security_rules
            .par_iter()
            .flat_map(|rule| rule.check(project, &taint_results))
            .collect();

        SecurityReport {
            findings,
            owasp_coverage: self.calculate_owasp_coverage(&findings),
            risk_score: self.calculate_risk_score(&findings),
        }
    }
}

// Example: SQL Injection Rule
pub struct SQLInjectionRule {
    sink_patterns: Vec<Regex>,
    source_patterns: Vec<Regex>,
    sanitization_patterns: Vec<Regex>,
}

impl StaticAnalysisRule for SQLInjectionRule {
    fn check(&self, context: &AnalysisContext) -> Vec<Finding> {
        let sources = context.dfg.find_taint_sources()
            .filter(|s| self.is_user_input(s))
            .collect::<Vec<_>>();

        if sources.is_empty() {
            return vec![];
        }

        let taint_flow = context.dfg.taint_analysis(&sources);
        let sinks = context.dfg.find_sinks();

        taint_flow.check_violations(&sinks, &self.sanitization_patterns)
    }
}
```

### 4. Software Composition Analysis (SCA)

**Prop√≥sito:** An√°lisis de dependencias y CVEs

**Arquitectura:**

```rust
pub struct SCAEngine {
    dependency_resolvers: HashMap<Ecosystem, Box<dyn DependencyResolver>>,
    cve_scanner: CVEScanner,
    sbom_generator: SBOMGenerator,
    license_checker: LicenseChecker,
}

pub struct DependencyResolver {
    ecosystem: Ecosystem,        // npm, cargo, go mod, pip
    lockfile_parser: LockfileParser,
    version_resolver: VersionResolver,
}

impl SCAEngine {
    pub async fn analyze(&self, project: &Project) -> SCAResult {
        // 1. Resolve dependencies
        let dependencies = self.resolve_all_dependencies(project).await?;

        // 2. Check CVEs
        let cve_findings = self.cve_scanner.scan(&dependencies).await?;

        // 3. Generate SBOM
        let sbom = self.sbom_generator.generate(&dependencies)?;

        // 4. Check licenses
        let license_info = self.license_checker.check(&dependencies)?;

        SCAResult {
            dependencies,
            cve_findings,
            sbom,
            license_info,
        }
    }
}
```

### 5. Data Layer

**Database Schema:**

```sql
-- Projects
CREATE TABLE projects (
    id UUID PRIMARY KEY,
    name VARCHAR NOT NULL,
    repository_url VARCHAR,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Analysis Results
CREATE TABLE analysis_runs (
    id UUID PRIMARY KEY,
    project_id UUID REFERENCES projects(id),
    commit_hash VARCHAR,
    analysis_date TIMESTAMP DEFAULT NOW(),
    total_issues INTEGER,
    critical_issues INTEGER,
    major_issues INTEGER,
    coverage_percentage DECIMAL(5,2),
    technical_debt_hours INTEGER
);

-- Issues/Findings
CREATE TABLE issues (
    id UUID PRIMARY KEY,
    analysis_id UUID REFERENCES analysis_runs(id),
    file_path VARCHAR NOT NULL,
    line_number INTEGER,
    rule_id VARCHAR NOT NULL,
    severity issue_severity NOT NULL,
    message TEXT,
    debt_hours INTEGER
);

-- Dependencies
CREATE TABLE dependencies (
    id UUID PRIMARY KEY,
    project_id UUID REFERENCES projects(id),
    name VARCHAR NOT NULL,
    version VARCHAR NOT NULL,
    ecosystem VARCHAR NOT NULL,
    is_direct BOOLEAN DEFAULT TRUE
);

-- CVEs
CREATE TABLE cve_findings (
    id UUID PRIMARY KEY,
    dependency_id UUID REFERENCES dependencies(id),
    cve_id VARCHAR NOT NULL,
    severity issue_severity NOT NULL,
    cvss_score DECIMAL(3,1),
    fixed_version VARCHAR
);
```

---

## üöÄ Patrones Arquitect√≥nicos

### 1. Hexagonal Architecture

**Ports (Interfaces):**

```rust
// Port: External API
pub trait AnalysisPort {
    async fn analyze_project(&self, project: &Project) -> Result<AnalysisResult>;
    async fn scan_dependencies(&self, project: &Project) -> Result<SCAResult>;
}

// Port: Storage
pub trait ProjectRepository {
    async fn save(&self, project: &Project) -> Result<()>;
    async fn get(&self, id: ProjectId) -> Result<Project>;
}

// Adapters
pub struct PostgresProjectRepository {
    db: Database,
}

#[async_trait]
impl ProjectRepository for PostgresProjectRepository {
    async fn save(&self, project: &Project) -> Result<()> {
        sqlx::query!("INSERT INTO projects ...")
            .execute(&self.db)
            .await?;
        Ok(())
    }
}
```

### 2. Actor Model para Parallel Processing

```rust
// Worker Actors
pub struct AnalysisWorker {
    rule_engine: Arc<RuleEngine>,
    language_analyzer: Arc<dyn LanguageAnalyzer>,
}

impl Actor for AnalysisWorker {
    type Message = AnalyzeFile;

    async fn handle(&mut self, msg: AnalyzeFile) -> Result<(), MailboxError> {
        let result = self.language_analyzer.parse(&msg.source_code)?;
        let cfg = self.language_analyzer.build_cfg(&result)?;
        let findings = self.rule_engine.evaluate(&result, &msg.context)?;

        msg.sender.send(findings).ok();
        Ok(())
    }
}

// Supervisor
pub struct AnalysisSupervisor {
    workers: Vec<Addr<AnalysisWorker>>,
    task_sender: mpsc::Sender<AnalyzeFile>,
}

impl AnalysisSupervisor {
    pub async fn analyze_project(&self, project: &Project) -> AnalysisResult {
        let files = self.collect_project_files(project);

        let futures = files.into_iter().map(|file| {
            self.task_sender.send(AnalyzeFile { file, sender: /* ... */ })
        });

        // Fan-out to workers
        futures::future::join_all(futures).await;

        // Collect results
        self.collect_results()
    }
}
```

### 3. Event-Driven Architecture

```rust
// Events
#[derive(Debug, Clone)]
pub enum AnalysisEvent {
    AnalysisCompleted(AnalysisId),
    SecurityIssueFound(IssueId, Severity),
    CVEFound(CVEInfo),
    QualityGateFailed(QualityGateId),
    CoverageDropped(CoverageDelta),
}

// Event Bus
pub struct EventBus {
    subscribers: Arc<RwLock<HashMap<String, Vec<Box<dyn EventSubscriber>>>>>,
}

impl EventBus {
    pub async fn publish(&self, event: &AnalysisEvent) {
        let topic = event.topic_name();
        let subscribers = self.subscribers.read().unwrap();

        if let Some(subs) = subscribers.get(&topic) {
            for subscriber in subs {
                subscriber.handle(event).await;
            }
        }
    }
}

// Subscribers
pub struct SlackNotifier {
    client: SlackClient,
}

#[async_trait]
impl EventSubscriber for SlackNotifier {
    async fn handle(&self, event: &AnalysisEvent) {
        if let AnalysisEvent::SecurityIssueFound(issue, severity) = event {
            if severity >= Severity::Critical {
                self.client.send_alert(issue).await;
            }
        }
    }
}
```

---

## üìä Performance y Escalabilidad

### Benchmarks Target vs SonarQube

| Componente | SonarQube | hodei-scan v2.0 | Mejora |
|------------|-----------|-----------------|--------|
| **Parser (100K LOC)** | 120s | 30s | **4x** |
| **CFG Build** | 60s | 15s | **4x** |
| **DFA Analysis** | 90s | 20s | **4.5x** |
| **Rule Evaluation** | 10-20ms | <2ms | **5-10x** |
| **Full Analysis (1M LOC)** | 30 min | 15 min | **2x** |
| **Peak Memory** | 4GB | 800MB | **5x** |
| **Rule Throughput** | 1M/hr | 3M/hr | **3x** |

### Optimizations

**1. Paralelizaci√≥n con Rayon**

```rust
// Parallel file processing
use rayon::prelude::*;

pub async fn analyze_project_parallel(project: &Project) -> AnalysisResult {
    let files = collect_files(project);

    let results: Vec<AnalysisResult> = files
        .par_iter()  // Parallel iterator
        .map(|file| analyze_file(file))
        .collect();

    merge_results(results)
}
```

**2. In-Memory Caching**

```rust
// LRU Cache con Redis backend
pub struct AnalysisCache {
    local_cache: Arc<Mutex<LruCache<FileId, CachedAnalysis>>>,
    redis: Arc<Redis>,
    ttl: Duration,
}

impl AnalysisCache {
    pub fn get(&self, file_id: &FileId) -> Option<CachedAnalysis> {
        // Check local cache first
        if let Some(result) = self.local_cache.lock().unwrap().get(file_id) {
            return Some(result.clone());
        }

        // Check Redis
        let key = format!("analysis:{}", file_id);
        if let Some(cached) = self.redis.get(&key).await? {
            // Populate local cache
            self.local_cache.lock().unwrap().put(file_id.clone(), cached.clone());
            Some(cached)
        } else {
            None
        }
    }
}
```

**3. Streaming Processing**

```rust
// Process large files in chunks
pub async fn analyze_large_file(
    &self,
    file: &LargeFile
) -> Result<AnalysisResult> {
    let mut stream = file.chunks(1000);  // 1000 lines at a time
    let mut partial_results = Vec::new();

    while let Some(chunk) = stream.next().await {
        let result = self.analyze_chunk(&chunk)?;
        partial_results.push(result);
    }

    Ok(merge_partial_results(partial_results))
}
```

---

## üîí Seguridad

### WASM Sandbox

```rust
pub struct WASMSandbox {
    engine: wasmtime::Engine,
    linker: wasmtime::Linker,
}

impl WASMSandbox {
    pub fn new() -> Result<Self> {
        let engine = wasmtime::Engine::new(
            wasmtime::Config::new()
                .cranelift_opt_level(wasmtime::OptLevel::Speed)
                .max_wasm_stack(1024 * 1024)  // 1MB stack limit
        )?;

        let mut linker = wasmtime::Linker::new(&engine);

        // Restrict imports
        linker.func_wrap("env", "print", |_: wasmtime::Val| {
            // No-op - no external I/O
        })?;

        Ok(WASMSandbox { engine, linker })
    }

    pub fn execute_rule(
        &self,
        wasm_bytes: &[u8],
        context: &AnalysisContext
    ) -> Result<Vec<Finding>, WASMError> {
        let module = wasmtime::Module::new(&self.engine, wasm_bytes)?;
        let mut store = wasmtime::Store::new(&self.engine, context);

        let instance = self.linker.instantiate(&mut store, &module)?;
        let find_violations = instance.get_typed_func::<(), Vec<wasmtime::Val>>(&mut store, "find_violations")?;

        let results = find_violations.call(&mut store, ())?;

        // Parse results
        Ok(self.parse_results(results))
    }
}
```

### Audit Logging

```rust
pub struct AuditLogger {
    event_bus: EventBus,
    database: Database,
}

impl AuditLogger {
    pub async fn log_analysis(&self, analysis: &Analysis) {
        let event = AuditEvent {
            timestamp: Utc::now(),
            user_id: analysis.user_id,
            action: "ANALYSIS_RUN".to_string(),
            resource: analysis.project_id.to_string(),
            details: json!({
                "files_analyzed": analysis.files.len(),
                "issues_found": analysis.total_issues,
                "duration_ms": analysis.duration.as_millis()
            }),
        };

        // Store in database
        sqlx::query!("INSERT INTO audit_log ...")
            .execute(&self.database)
            .await?;

        // Publish event
        self.event_bus.publish(&event).await;
    }
}
```

---

## üìà Monitoreo y Observabilidad

### Metrics

```rust
use metrics::{counter, gauge, histogram};

pub struct MetricsCollector {
    analysis_duration: Histogram,
    active_analyses: Gauge,
    issues_found: Counter,
    memory_usage: Gauge,
}

impl MetricsCollector {
    pub fn record_analysis_duration(&self, duration: Duration) {
        self.analysis_duration.record(duration.as_secs_f64());
    }

    pub fn increment_active_analyses(&self) {
        self.active_analyses.increment(1.0);
    }

    pub fn record_issues_found(&self, count: usize, severity: Severity) {
        self.issues_found.with_label_values(&[severity.as_str()]).inc();
    }
}
```

### Health Checks

```rust
pub struct HealthChecker {
    database: Database,
    cve_database: CVEDatabase,
    storage: Storage,
}

impl HealthChecker {
    pub async fn check_all(&self) -> HealthStatus {
        let checks = vec![
            self.check_database().await,
            self.check_cve_database().await,
            self.check_storage().await,
        ];

        let all_healthy = checks.iter().all(|c| c.healthy);

        HealthStatus {
            overall: if all_healthy { "healthy" } else { "unhealthy" },
            checks,
        }
    }
}
```

---

## üõ†Ô∏è Deployment Architecture

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hodei-scan
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hodei-scan
  template:
    metadata:
      labels:
        app: hodei-scan
    spec:
      containers:
      - name: api
        image: hodei-scan:latest
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: hodei-secrets
              key: database-url
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
```

### Docker Configuration

```dockerfile
# Dockerfile
FROM rust:1.75-slim as builder

WORKDIR /app
COPY . .
RUN cargo build --release

FROM debian:bookworm-slim

RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /app/target/release/hodei-scan /usr/local/bin/
COPY --from=builder /app/config /etc/hodei-scan

EXPOSE 8080
CMD ["hodei-scan", "serve"]
```

---

## üîÑ Continuous Integration

### GitHub Actions Pipeline

```yaml
# .github/workflows/ci.yml
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
    - name: Run Tests
      run: cargo test --all -- --test-threads=1
    - name: Run Integration Tests
      run: cargo test --test integration -- --test-threads=1
    - name: Coverage
      run: |
        cargo install cargo-tarpaulin
        cargo tarpaulin --all --out xml

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Security Audit
      run: |
        cargo install cargo-audit
        cargo audit
    - name: License Check
      run: |
        cargo install cargo-license
        cargo license --json > licenses.json
```

---

## üìö ADRs (Architecture Decision Records)

### ADR-001: Cedar-Inspired Rule Engine

**Status:** Accepted
**Date:** 2025-11-10

**Context:** Necesitamos un motor de reglas r√°pido y determinista para static analysis.

**Decision:** Usar Cedar-inspired approach con:
- DSL declarativo para reglas
- Indexaci√≥n por node type
- Evaluaci√≥n paralela con Rayon
- WASM sandbox para extensibilidad

**Consequences:**
- ‚úÖ Performance: <2ms rule evaluation
- ‚úÖ Determinism: O(n) time complexity
- ‚úÖ Extensibility: WASM rules
- ‚ùå Complexity: Higher implementation complexity
- ‚ùå Learning curve: DSL to learn

### ADR-002: Elimination of LSP Dependency

**Status:** Accepted
**Date:** 2025-11-10

**Context:** Identificamos contradicci√≥n arquitect√≥nica entre LSP (IDE integration) y batch analysis.

**Decision:** Eliminar todas las referencias a LSPs del motor core, usar √∫nicamente:
- tree-sitter para parsing
- Motores sem√°nticos espec√≠ficos por lenguaje
- An√°lisis batch stateless

**Consequences:**
- ‚úÖ Arquitectura coherente
- ‚úÖ Performance optimizada para batch
- ‚úÖ Simplicidad
- ‚ùå No IDE real-time integration
- ‚ùå An√°lisis m√°s simple que CodeQL

### ADR-003: Hexagonal Architecture

**Status:** Accepted
**Date:** 2025-11-10

**Context:** Sistema complejo con m√∫ltiples integraciones externas.

**Decision:** Implementar Hexagonal Architecture con:
- Ports (interfaces) para external dependencies
- Adapters para implementaciones espec√≠ficas
- Dependency inversion para testability

**Consequences:**
- ‚úÖ Testability alta
- ‚úÖ Modularidad
- ‚úÖ Flexibility para cambiar implementaciones
- ‚ùå M√°s boilerplate
- ‚ùå Complejidad adicional

---

## üîÆ Roadmap T√©cnico

### Fase 1: MVP (Meses 1-6)
- [ ] 3 lenguajes (Rust, Go, TypeScript)
- [ ] Core engine sin LSP
- [ ] OWASP Top 10
- [ ] Performance 2x vs SonarQube

### Fase 2: Expansion (Meses 7-12)
- [ ] +3 lenguajes (Python, C++, Java)
- [ ] SCA engine completo
- [ ] Code coverage integration
- [ ] Quality gates

### Fase 3: Enterprise (Meses 13-24)
- [ ] Portfolio management
- [ ] PR decoration
- [ ] Enterprise features (RBAC, SSO)
- [ ] Compliance reporting

---

## üìû Contacto

**Chief Architect:** [A definir]
**Technical Lead:** [A definir]
**Architecture Review Board:** architecture@hodei-scan.dev
**Slack:** #hodei-scan-architecture

---

*√öltima actualizaci√≥n: 10 de noviembre de 2025*

**Next Document:** [TDD Methodology](./TDD_METHODOLOGY.md)
